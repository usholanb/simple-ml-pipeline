includes:
  - configs/constant_configs.yml


dataset:
  name: basket_dataset
  input_path: processed_data/dagnet_dataset

  data_loaders:
    train:
      batch_size: 100
      shuffle: true
      num_workers: 4

    valid:
      batch_size: 100
      shuffle: false
      num_workers: 4

    test:
      batch_size: 100
      shuffle: false
      num_workers: 4


model:
  name: dagnet
  tag: uan


# must be same names as parameters of the actual optimizer
optim:
  lr: 0.01
  weight_decay: 0.00001


# this inputs will go to your specific model __init__
special_inputs:

  num_workers: 4
  obs_len: 10
  pred_len: 40
  players: all  # choices=['atk', 'def', 'all'], help='Which players to use'

  lr_scheduler: false
  warmup: false
  wrmp_epochs: 50
  CE_weight: 0.01
  batch_size: 64

  clip: 10
  n_layers: 2  # Number of recurrent layers
  x_dim: 2
  h_dim: 64
  z_dim: 32
  g_dim: 90  # Dimension of the goal variables
  rnn_dim: 64  # Dimension of the recurrent layers

  seed: 128
  print_every_batch: 30
#  save_every: 10
#  eval_every: 20
  num_samples: 20  # Number of samples for evaluation

  graph_model: gcn  #  choices=['gat','gcn'], help='Graph type'
  graph_hid: 8  #  help='Number of hidden units'
  sigma: 1.2  # Sigma value for similarity matrix

  # Type of adjacency matrix:
  # 0 (fully connected graph),
  # 1 (distances similarity matrix),
  # 2 (knn similarity matrix).
  adjacency_type: 1

  top_k_neigh: None
  n_heads: 4  # Number of heads for graph attention network
  alpha: 0.2  # Negative steep for the Leaky-ReLU activation

  transformers: goals_ohe


trainer:
  name: torch_trainer3
  optim: Adam
  loss: dagnet_loss
  save: 1
  epochs: 3
  log_valid_every: 10

  tune:
    resources_per_trial:
      cpu: 2
      gpu: 0

#  metrics:
#    - mse_loss

  grid_metric:
    name: train_DagnetLoss
    mode: min




#features_list:










